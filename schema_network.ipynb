{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics](https://arxiv.org/pdf/1706.04317.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Препрцессинг данных:\n",
    "![ ](img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Схемы:\n",
    "Для момента $t$ набор схем W предсказывает j-ые атрибуты или награду в мемент $t+1$ :\n",
    "\n",
    "$$ y =  \\overline{\\overline{X} W}\\vec{1} = f_W(X)$$\n",
    "\n",
    "где сложение - логическое или, черта -  отрицание. Значение W - решение оптимизационной задачи:\n",
    "$$ min_{W} \\frac{1}{RM}|y - f_W(X)|_1 + C|W|_1$$\n",
    "Алгоритм приближенного решения предлагается в конце статьи.\n",
    "![ ](img4.png)\n",
    "Схемы задают граф переходов между состояниями, где все возможные переходы равновероятны:\n",
    "![ ](ing3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Планирование действий:\n",
    "Авторы используют max-product belief propagation (MPBP), который состоит из стадий:\n",
    "* **Анализ возможных достижений:**\n",
    "* **Выбор цели с положительной наградой:**\n",
    "* **Избегание состояний с отрицательной наградой:**\n",
    "* **Backtracking:**\n",
    "\n",
    "Алгоритм описан в [Planning by Probabilistic Inference](https://pdfs.semanticscholar.org/01aa/f5722c0bcb06d536a359e4a2223c7755e8f3.pdf?_ga=2.89309403.1549993353.1563450871-1418804146.1563450871) (вроде он, но в статье он так не называется)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общий алгоритм:\n",
    "1. **Преобразование входного изображения:** получить набор сущностей с бинарными атрибутами, используя, например, [autoencoders](https://arxiv.org/pdf/1609.05518.pdf).\n",
    "2. **Преобразование данных:** построить матрицу $X^t$: где i-ый столбец состоит из атрибутов i-ой сущности и атрибутов ее соседей для заданного радиуса в момент $t$\n",
    "3. **Обучение:** Пусть на предыдущем шаге было совершено действие а, тогда:\n",
    "    *  $T = \\{t:$ в момент $t$ было совершено $a\\}$   <font color='orange'>(о том, что для каждого действия своя схема упоминается только в секции 3.2, но это в целом логично)</font>\n",
    "    *  $X = \\begin{pmatrix} X_{t_1} \\\\ ... \\\\ X_{t_{|T|}} \\end{pmatrix}, t_i \\in T$\n",
    "    *  для каждого атрибута $j$: $y =  \\begin{pmatrix} \\alpha_{1j}^{t_1 + 1} \\\\ ... \\\\ \\alpha_{Mj}^{t_{|T|}+1} \\end{pmatrix}$ - атрибуты после действия  $a$ <font color='orange'>(я так поняла, что аналогично предсказываются награды)</font>\n",
    "    *  поиск оптимальных схем: $W = argmin_{W} \\frac{1}{RM}|y - f_W(X)|_1 + C|W|_1$\n",
    "        *  $x_n| f_W(x_n)=0$\n",
    "        * добавим один $x_n$ в множество $S$\n",
    "        * добавление схемы:\n",
    "         $$min_w \\sum  \\limits_{n:y_n =1}(1−x_n)w$$\n",
    "         $$(1 − x_n)w >1|y_n=0$$\n",
    "         $$(1 − x_n)w = 0| x_n \\in S$$\n",
    "        * добавим все $x_n$ $| (1-x_n)w = 0 $ в $S$\n",
    "        * упрощение схемы:\n",
    "         $$min_w|w|_1$$\n",
    "         $$(1 − x_n)w >1|y_n=0$$\n",
    "         $$(1 − x_n)w = 0| x_n \\in S$$\n",
    "        * если полученная схема не бинарна, то повторить пердыдущий шаг для ненулевых элеменотов схемы, используя бинарную оптимизацию \n",
    "4. **Выбор дейстовия [Planning by Probabilistic Inference]:**\n",
    "    * $s_1$ - текущее состояние, $T$ - временное окно \n",
    "    * Поиск достижимого состояния с положительной наградой (Forward Pass):\n",
    "        * для всех $ n = 2, ..., T$: \n",
    "         $$q_n(s′,a′)= \\sum\\limits_{a,s}q_n(s′,a′ |s,a)q_{n−1}(s,a)$$\n",
    "         $$q_n(a′ | a) = \\frac{\\sum\\limits_{s, s'}q_n(s', a'|s ,a)q_{n-1}(s, a)}{ \\sum_sq_{n−1}(s,a)}$$\n",
    "        * среди всех положительных  наград выберем ту, которая достигаеся раньше и присвоим ей 1\n",
    "        * присвоим 0 тем отрицательным наргадам, для которых такая позиция достижима \n",
    "    * Поиск последовательности действий, ведущей к желаемому состоянию:\n",
    "        * Комбинация Viterby и поиска в глубину <font color='orange'>(Тут не совсем понятно: в данном алгоритме нет вероятности, как таковой (действие и состояние однозначно задают следующее состояние), а все условные вероятности просто индикаторы. При этом отрицательной награды хочется избегать на всем пути следования. Так что витерби, может, стоит применять как раз для минимизации наказания? Эта часть алгоритма вообще очень расплывчато описана.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
